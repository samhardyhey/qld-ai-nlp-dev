{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 8, 6\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"pastel\", 12)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_char(string):\n",
    "    # new line/tab delimeters and barstad ascii variants\n",
    "    replacements = [\n",
    "        (\"\\n\", \" \"),\n",
    "        (\"\\r\", \" \"),\n",
    "        (\"\\t\", \" \"),\n",
    "        (\"\\\\n\", \" \"),\n",
    "        (\"\\\\r\", \" \"),\n",
    "        (\"\\\\t\", \" \"),\n",
    "    ]\n",
    "    for e in replacements:\n",
    "        string = string.replace(*e)\n",
    "    return \" \".join(string.split())\n",
    "\n",
    "\n",
    "def replace_malformed_hex(string):\n",
    "    string = string.replace(\"\\\\\", \" \")\n",
    "    string = re.sub(\"x[0-9]{2,3}\", \" \", string)\n",
    "    string = re.sub(\"xe[0-9]\", \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def multi_process_spacy_docs(texts, nlp, n_process=None, batch_size=256):\n",
    "    if not n_process:\n",
    "        n_process = cpu_count()\n",
    "    # 1.0 ensure that multi-processing isn't used frivolously\n",
    "    num_docs = len(texts)\n",
    "    if num_docs <= 100:\n",
    "        return [nlp(e) for e in texts]\n",
    "\n",
    "    # 2.0 batch documents, ensure against memory overflows\n",
    "    iteration_size = batch_size * n_process\n",
    "    total_iterations = int(len(texts) / iteration_size)\n",
    "\n",
    "    if total_iterations < 1:\n",
    "        # redefine batch size to ensure best spread across CPU cores\n",
    "        optimal_batch_size = int(len(texts) / n_process)\n",
    "        return list(nlp.pipe(texts, batch_size=optimal_batch_size, n_process=n_process))\n",
    "    else:\n",
    "        # otherwise, iterate through large chunks of documents\n",
    "        iteration_splits = np.array_split(texts, total_iterations)\n",
    "        docs = []\n",
    "        for split in iteration_splits:\n",
    "            docs.extend(\n",
    "                list(nlp.pipe(split, batch_size=batch_size, n_process=n_process))\n",
    "            )\n",
    "        return docs\n",
    "\n",
    "\n",
    "def downsample_frame(df, n):\n",
    "    # either n or max records in frame\n",
    "    if df.shape[0] < n:\n",
    "        return df\n",
    "    else:\n",
    "        return df.sample(n=n, random_state=42)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis and Table Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"otso is a machine learning company that specialises in the analysis of unstructured text data using state of the art natural language processing and artificial intelligence technology.\"\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token frame\n",
    "pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            \"text\": e.text,\n",
    "            \"start_char\": e.idx,\n",
    "            \"end_char\": e.idx + len(e),\n",
    "            \"is_digit\": e.is_digit,\n",
    "            \"is_punct\": e.is_punct,\n",
    "        }\n",
    "        for e in doc\n",
    "    ]\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render dependency parse\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\n",
    "    \"otso makes it easy to analyse all of your customer feedback and media mentions, from any source, at scale.\"\n",
    ")\n",
    "displacy.render(\n",
    "    doc,\n",
    "    style=\"dep\",\n",
    "    jupyter=True,\n",
    "    options={\"distance\": 80, \"collapse_phrases\": True, \"bg\": \"#ffffff\"},\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pos frame\n",
    "pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"text\": e.text, \"pos\": e.pos_, \"lemma\": e.lemma_, \"embedding_sentence\": e.sent}\n",
    "        for e in doc\n",
    "    ]\n",
    ").head(10)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.noun_chunks)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noun chunk frame\n",
    "pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"noun_chunk\": e.text, \"root\": e.root.text, \"root_lemma\": e.root.lemma_}\n",
    "        for e in doc.noun_chunks\n",
    "    ]\n",
    ").head(10)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_doc = \" \".join(\n",
    "    [\n",
    "        \"With a lot of machine learning providers, it can feel like there's not a lot of room for flexibility, or specialisation to suit your needs. We built otso to address many of the shortfalls we saw in existing natural language systems, meaning it is built to work with a range of different use-cases, and can also be tuned and specialised to suit almost any natural language need.\",\n",
    "        \"otso makes it easy to analyse all of your customer feedback and media mentions, from any source, at scale. Discover new insights and explore relationships within your world of data, powered by the latest advances in AI.\",\n",
    "        \"otso can ingest your data in many different ways. Simply drag and drop your data files, integrate with external data partners, or work with our team to build a custom solution.\",\n",
    "    ]\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(large_doc)\n",
    "displacy.render(\n",
    "    doc,\n",
    "    style=\"ent\",\n",
    "    jupyter=True,\n",
    "    options={\"distance\": 80, \"collapse_phrases\": True, \"bg\": \"#ffffff\"},\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fan_ents_single(doc):\n",
    "    # fan/tabulate all eities within a single doc\n",
    "    entities = []\n",
    "    for e in doc.ents:\n",
    "        # collect eity annotations\n",
    "        entities.append(\n",
    "            {\n",
    "                \"embedding_sentence\": e.sent.text,\n",
    "                \"entity\": e.text,\n",
    "                \"entity_label\": e.label_,\n",
    "                \"entity_lemma\": e.lemma_.lower(),\n",
    "                \"entity_pos\": e.root.pos_,\n",
    "                \"start\": e.start_char - e.sent.start_char,\n",
    "                \"end\": e.end_char - e.sent.start_char,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame.from_records(entities)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_ents_single(doc)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noun chunk frame\n",
    "pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"noun_chunk\": e.text, \"root\": e.root.text, \"root_lemma\": e.root.lemma_}\n",
    "        for e in doc.noun_chunks\n",
    "    ]\n",
    ").head(10)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_process_spacy_docs(texts, nlp, n_process=None, batch_size=256):\n",
    "    if not n_process:\n",
    "        n_process = cpu_count()\n",
    "    # 1.0 ensure that multi-processing isn't used frivolously\n",
    "    num_docs = len(texts)\n",
    "    if num_docs <= 100:\n",
    "        return [nlp(e) for e in texts]\n",
    "\n",
    "    # 2.0 batch documents, ensure against memory overflows\n",
    "    iteration_size = batch_size * n_process\n",
    "    total_iterations = int(len(texts) / iteration_size)\n",
    "\n",
    "    if total_iterations < 1:\n",
    "        # redefine batch size to ensure best spread across CPU cores\n",
    "        optimal_batch_size = int(len(texts) / n_process)\n",
    "        return list(nlp.pipe(texts, batch_size=optimal_batch_size, n_process=n_process))\n",
    "    else:\n",
    "        # otherwise, iterate through large chunks of documents\n",
    "        iteration_splits = np.array_split(texts, total_iterations)\n",
    "        docs = []\n",
    "        for split in iteration_splits:\n",
    "            docs.extend(\n",
    "                list(nlp.pipe(split, batch_size=batch_size, n_process=n_process))\n",
    "            )\n",
    "        return docs\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            (pd.read_csv(e, usecols=[\"articleText\", \"category\", \"date\"]))\n",
    "            for e in glob.glob(\"../datasets/publications/**\", recursive=True)\n",
    "            if \".csv\" in e\n",
    "        ],\n",
    "        sort=True,\n",
    "    ).pipe(\n",
    "        lambda x: x[x.articleText.apply(lambda y: True if type(y) == str else False)]\n",
    "    )\n",
    "    # use a subset of documents to prototype\n",
    "    .sample(n=5000, random_state=42)\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark wall times across different core configs\n",
    "wall_times = []\n",
    "for num_cores in [8, 16, 32]:\n",
    "    before = timeit.default_timer()\n",
    "    docs = multi_process_spacy_docs(\n",
    "        df.head(5000).articleText, nlp, n_process=num_cores, batch_size=512\n",
    "    )\n",
    "    after = timeit.default_timer()\n",
    "    wall_times.append({\"cores\": num_cores, \"wall_time\": after - before})\n",
    "    print(\"finished \", num_cores)\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 8, 8\n",
    "sns.lineplot(x=\"cores\", y=\"wall_time\", data=pd.DataFrame.from_records(wall_times))\n",
    "plt.title(\"Effect of Multi-core Spacy Pipe Operations, 5000 Documents\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark wall times across different pipeline configs\n",
    "pipeline_configs = {\n",
    "    \"tokenisation_ner_parser_tagger\": [],\n",
    "    \"tokenisation_ner_parser\": [\"tagger\"],\n",
    "    \"tokenisation_ner\": [\"tagger\", \"parser\"],\n",
    "    \"tokenisation\": [\"tagger\", \"parser\", \"ner\"],\n",
    "}\n",
    "wall_times = []\n",
    "\n",
    "for k, v in pipeline_configs.items():\n",
    "    before = timeit.default_timer()\n",
    "    docs = list(\n",
    "        nlp.pipe(df.head(2000).articleText, batch_size=32, n_process=16, disable=v)\n",
    "    )\n",
    "    after = timeit.default_timer()\n",
    "    wall_times.append({\"pipeline_config\": k, \"wall_time\": after - before})\n",
    "    print(\"finished\", k)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"] = 8, 8\n",
    "sns.lineplot(\n",
    "    x=\"pipeline_config\", y=\"wall_time\", data=pd.DataFrame.from_records(wall_times)\n",
    ")\n",
    "plt.title(\"Effect of Simplifying Spacy Pipe Operations, 2000 Documents, 16 Cores\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

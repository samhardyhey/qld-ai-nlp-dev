{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pandarallel import pandarallel\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 8, 6\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"pastel\", 12)\n",
    "\n",
    "# pandarallel.initialize(shm_size_mb=2000, nb_workers=64)\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_char(string):\n",
    "    # new line/tab delimeters and barstad ascii variants\n",
    "    replacements = [\n",
    "        (\"\\n\", \" \"),\n",
    "        (\"\\r\", \" \"),\n",
    "        (\"\\t\", \" \"),\n",
    "        (\"\\\\n\", \" \"),\n",
    "        (\"\\\\r\", \" \"),\n",
    "        (\"\\\\t\", \" \"),\n",
    "    ]\n",
    "    for e in replacements:\n",
    "        string = string.replace(*e)\n",
    "    return ' '.join(string.split())\n",
    "\n",
    "def replace_malformed_hex(string):\n",
    "    string = string.replace(\"\\\\\", \" \")\n",
    "    string = re.sub(\"x[0-9]{2,3}\", \" \", string)\n",
    "    string = re.sub(\"xe[0-9]\", \" \", string)\n",
    "    return string\n",
    "\n",
    "def multi_process_spacy_docs(texts, nlp, n_process=None, batch_size=256):\n",
    "    if not n_process:\n",
    "        n_process = cpu_count()\n",
    "    # 1.0 ensure that multi-processing isn't used frivolously\n",
    "    num_docs = len(texts)\n",
    "    if num_docs <= 100:\n",
    "        return [nlp(e) for e in texts]\n",
    "\n",
    "    # 2.0 batch documents, ensure against memory overflows\n",
    "    iteration_size = batch_size * n_process\n",
    "    total_iterations = int(len(texts) / iteration_size)\n",
    "\n",
    "    if total_iterations < 1:\n",
    "        # redefine batch size to ensure best spread across CPU cores\n",
    "        optimal_batch_size = int(len(texts) / n_process)\n",
    "        return list(\n",
    "            nlp.pipe(texts, batch_size=optimal_batch_size, n_process=n_process)\n",
    "        )\n",
    "    else:\n",
    "        # otherwise, iterate through large chunks of documents\n",
    "        iteration_splits = np.array_split(texts, total_iterations)\n",
    "        docs = []\n",
    "        for split in iteration_splits:\n",
    "            docs.extend(\n",
    "                list(nlp.pipe(split, batch_size=batch_size, n_process=n_process))\n",
    "            )\n",
    "        return docs\n",
    "    \n",
    "def downsample_frame(df, n):\n",
    "    # either n or max records in frame\n",
    "    if df.shape[0] < n:\n",
    "        return df\n",
    "    else:\n",
    "        return df.sample(n=n, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "df = (pd.concat([(pd.read_csv(e, usecols=['articleText', 'category', 'date'])\n",
    "                  .pipe(lambda x: downsample_frame(x, 100))\n",
    "                  ) for e in glob.glob('../datasets/publications/**', recursive=True) if '.csv' in e], sort=True)\n",
    "      .pipe(lambda x: x[x.articleText.apply(lambda y: True if type(y) == str else False)])\n",
    "      # use a subset of documents to prototype\n",
    "      .sample(n=50000, random_state=42)\n",
    "      # remove tags\n",
    "      .assign(articleText=lambda x: x.articleText.parallel_apply(lambda y: BeautifulSoup(y, \"lxml\").text))\n",
    "      # replace malformed hex chars\n",
    "      .assign(articleText=lambda x: x.articleText.parallel_apply(replace_malformed_hex))\n",
    "      # remove excessive chars\n",
    "      .assign(articleText=lambda x: x.articleText.parallel_apply(remove_excess_char))\n",
    "      # drop duplicates on articleText\n",
    "      .drop_duplicates(subset=['articleText'])\n",
    "      # get doc len\n",
    "      .assign(doc_len=lambda x: x.articleText.parallel_apply(lambda y: len(y.split(' '))))\n",
    "      # assign spacy doc\n",
    "      .assign(spacy_doc=lambda x: multi_process_spacy_docs(x.articleText, nlp, batch_size=128))\n",
    "      .assign(doc_uuid=lambda x: [uuid.uuid4().hex for e in range(x.shape[0])])\n",
    "      .assign(date=lambda x: x.date.parallel_apply(pd.to_datetime))\n",
    "      # pull out some corpus-level stats\n",
    "      .assign(num_sentences=lambda x: x.spacy_doc.apply(lambda y: len(list(y.sents))))\n",
    "      .assign(num_tokens=lambda x: x.spacy_doc.apply(lambda y: len(list(y))))\n",
    "      .assign(pub_year=lambda x: x.date.apply(lambda y: y.year))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Entity and Noun Chunk Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fan_ents_all(df, spacy_doc, doc_uuid):\n",
    "    def fan_ents_single(doc, doc_uuid):\n",
    "        # fan/tabulate all eities within a single doc\n",
    "        entities = []\n",
    "        for e in doc.ents:\n",
    "            # collect eity annotations\n",
    "            entities.append(\n",
    "                {\n",
    "                    \"embedding_sentence\": e.sent.text,\n",
    "                    \"entity\": e.text,\n",
    "                    \"entity_label\": e.label_,\n",
    "                    \"entity_lemma\": e.lemma_.lower(),\n",
    "                    \"entity_pos\": e.root.pos_,\n",
    "                    \"start\": e.start_char - e.sent.start_char,\n",
    "                    \"end\": e.end_char - e.sent.start_char,\n",
    "                    \"doc_uuid\": doc_uuid,\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame.from_records(entities)\n",
    "\n",
    "    return pd.concat(df.apply(lambda x: fan_ents_single(x.spacy_doc, x.doc_uuid), axis=1).tolist())\n",
    "\n",
    "def fan_noun_chunks_all(df, spacy_doc, doc_uuid):\n",
    "    # compile all noun chunks within a single spacy doc\n",
    "    def fan_noun_chunks_single(doc, doc_uuid, min_num_tokens=2):\n",
    "        noun_chunks = []\n",
    "        for e in doc.noun_chunks:\n",
    "            if len(e) >= min_num_tokens:\n",
    "                noun_chunks.append(\n",
    "                    {\n",
    "                        \"embedding_seence\": e.sent.text,\n",
    "                        \"noun_chunk\": e.text,\n",
    "                        \"root\": e.root,\n",
    "                        \"root_lemma\": e.root.lemma_.lower(),\n",
    "                        \"root_pos\": e.root.pos_,\n",
    "                        \"doc_uuid\": doc_uuid,\n",
    "                    }\n",
    "                )\n",
    "        return pd.DataFrame.from_records(noun_chunks)\n",
    "\n",
    "    return pd.concat(df.apply(lambda x: fan_noun_chunks_single(x.spacy_doc, x.doc_uuid), axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noun_chunk_frame = fan_noun_chunks_all(df, 'spacy_doc','doc_uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "entity_frame = fan_ents_all(df, 'spacy_doc', 'doc_uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_meta_information_frame = pd.read_csv('../datasets/cache/50000_docs_meta_information.csv')\n",
    "entity_frame = pd.read_csv('../datasets/cache/50000_docs_entity_frame.csv')\n",
    "noun_chunks_frame = pd.read_csv('../datasets/cache/50000_noun_chunk_frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_entity_types = ['PERSON',\n",
    "                            'ORG',\n",
    "                            'GPE',\n",
    "                            'NORP',\n",
    "                            'WORK_OF_ART',\n",
    "                            'QUANTITY',\n",
    "                            'PRODUCT',\n",
    "                            'FAC',\n",
    "                            'LOC',\n",
    "                            'EVENT',\n",
    "                            'LAW',\n",
    "                           ]\n",
    "\n",
    "pre_fed_doc_uuids = (document_meta_information_frame\n",
    "                     .query('pub_year <= 1900')\n",
    "                     .doc_uuid\n",
    "                     )\n",
    "\n",
    "# top 20 most mentioned PERSON entities (with some error..)\n",
    "(entity_frame\n",
    " .pipe(lambda x: x[x.doc_uuid.isin(pre_fed_doc_uuids)])\n",
    "#  .pipe(lambda x: x[x.entity_label.isin(interesting_entity_types)])\n",
    " .query('entity_label == \"PERSON\"')\n",
    " .groupby(by=['entity_label', 'entity_lemma'])\n",
    " .size()\n",
    " .sort_values(ascending=False)\n",
    " .reset_index().rename(columns={0: 'frequency'})\n",
    " .head(20)\n",
    " )\n",
    "\n",
    "# compare/contrast with top 20 noun chunks\n",
    "(noun_chunks_frame\n",
    " .pipe(lambda x: x[x.doc_uuid.isin(pre_fed_doc_uuids)])\n",
    " .groupby(by=['root_lemma'])\n",
    " .size()\n",
    " .sort_values(ascending=False)\n",
    " .reset_index().rename(columns={0: 'frequency'})\n",
    " .head(20)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_frame.query('entity_lemma == \"stabling\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_frame.query('entity_lemma == \"sheep\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
